"""
===============================================================================
                        üöÄ CASIMAGE ‚Äì Pipeline Professionnel
                Extraction ‚Üí Nettoyage ‚Üí Star Schema ‚Üí Data Warehouse
-------------------------------------------------------------------------------
Auteurs :
    ‚Ä¢ KACEM Chayma
    ‚Ä¢ NECHI Zeinab
    ‚Ä¢ HAMMAMI Emir

Description :
    Pipeline complet :
        ‚úî Extraction ZIP
        ‚úî Parsing XML ‚Üí DataFrame
        ‚úî Nettoyage avanc√© (Age, Sexe, Texte‚Ä¶)
        ‚úî Export CSV + Parquet
        ‚úî Construction du Star Schema
        ‚úî Chargement dans MySQL (XAMPP) ou SQLite fallback
===============================================================================
"""

# =============================================================================
# Imports
# =============================================================================

import os
import glob
import json
import shutil
import xmltodict
import pandas as pd
from dotenv import load_dotenv

from transform_utils import clean_xml_text, flatten_qcm
from xml_utils import unzip_to_folder, summarize_xml_structure
from enhanced_cleaning import apply_enhanced_cleaning
from dw_model import (
    build_star_schema,
    load_star_schema_sqlite,
    load_star_schema_mysql
)
from config import (
    DATA_DIR, EXTRACT_DIR, OUTPUT_DIR,
    CSV_PATH, DB_PATH
)


# =============================================================================
# Initialisation
# =============================================================================

load_dotenv()
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Nettoyage du dossier EXTRACT
shutil.rmtree(EXTRACT_DIR, ignore_errors=True)


# =============================================================================
# 1Ô∏è‚É£ Extraction ZIP
# =============================================================================

zip_files = glob.glob(os.path.join(DATA_DIR, "*.zip"))
if not zip_files:
    raise SystemExit("‚ùå Aucun fichier ZIP trouv√© dans data/.")

zip_path = zip_files[0]
print(f"üì¶ ZIP d√©tect√© : {os.path.basename(zip_path)}")

xml_files = unzip_to_folder(zip_path, EXTRACT_DIR)
print(f"‚úÖ {len(xml_files)} fichiers XML extraits.\n")

if not xml_files:
    raise SystemExit("‚ùå Extraction √©chou√©e : aucun XML trouv√©.")


# =============================================================================
# 2Ô∏è‚É£ Analyse de structure XML
# =============================================================================

sample = xml_files[0]
summary = summarize_xml_structure(sample)

print(f"üîç Exemple analys√© : {os.path.basename(sample)}")
print(summary[:500], "...\n")


# =============================================================================
# 3Ô∏è‚É£ Lecture XML ‚Üí DataFrame
# =============================================================================

rows = []

for path in xml_files:
    try:
        with open(path, "r", encoding="iso-8859-1", errors="ignore") as f:
            raw = clean_xml_text(f.read())
            data = xmltodict.parse(raw)

        case = data.get("CASIMAGE_CASE", {})

        record = {k: v for k, v in case.items() if isinstance(v, str)}
        record["QCMs"] = flatten_qcm(case)
        record["SourceFile"] = os.path.basename(path)

        rows.append(record)

    except Exception as e:
        print(f"‚ö†Ô∏è Erreur parsing {os.path.basename(path)} : {e}")


if not rows:
    raise SystemExit("‚ùå Aucun XML valide n‚Äôa pu √™tre converti.")

df = pd.DataFrame(rows)
print(f"üìä {len(df)} lignes extraites avant nettoyage.\n")


# =============================================================================
# 4Ô∏è‚É£ Nettoyage basique
# =============================================================================

df.drop_duplicates(inplace=True)
df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
df.replace({"": None, " ": None}, inplace=True)


# =============================================================================
# 5Ô∏è‚É£ Nettoyage avanc√© (Age, Sexe, Texte‚Ä¶)
# =============================================================================

print("üß† Nettoyage avanc√©...")
df = apply_enhanced_cleaning(df)


# =============================================================================
# 6Ô∏è‚É£ Export CSV + Parquet
# =============================================================================

csv_path = CSV_PATH
parquet_path = os.path.join(OUTPUT_DIR, "casimage_clean.parquet")

df.to_csv(csv_path, index=False, encoding="utf-8")
df.to_parquet(parquet_path, index=False)

print("üíæ Export termin√© :")
print(f"‚Üí CSV      : {csv_path}")
print(f"‚Üí Parquet  : {parquet_path}\n")


# =============================================================================
# 7Ô∏è‚É£ Construction du Star Schema
# =============================================================================

print("‚≠ê Construction du Star Schema‚Ä¶")
star = build_star_schema(df)


# =============================================================================
# 8Ô∏è‚É£ Chargement dans Data Warehouse (MySQL ‚Üí fallback SQLite)
# =============================================================================

MYSQL_URI = os.getenv("MYSQL_URI", "").strip()

if MYSQL_URI:
    print("üåê Tentative de chargement dans MySQL (XAMPP)‚Ä¶")
    try:
        load_star_schema_mysql(star, MYSQL_URI)
        print("‚úÖ DW MySQL mis √† jour !")
    except Exception as e:
        print(f"‚ö†Ô∏è MySQL indisponible : {e}")
        print("‚û°Ô∏è Utilisation automatique de SQLite.")
        load_star_schema_sqlite(star, DB_PATH)
else:
    print("‚ÑπÔ∏è Aucun MYSQL_URI d√©fini ‚Üí SQLite utilis√©.")
    load_star_schema_sqlite(star, DB_PATH)

print(f"üè• Data Warehouse mis √† jour : {DB_PATH}\n")


# =============================================================================
# 9Ô∏è‚É£ Nettoyage final
# =============================================================================

shutil.rmtree(EXTRACT_DIR, ignore_errors=True)
print("üßπ Dossier EXTRACT supprim√©.")

print("\nüèÅ Pipeline CASIMAGE termin√© avec succ√®s.")
